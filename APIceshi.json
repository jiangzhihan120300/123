[
    {
        "logs":""hidden_size": 4096,"intermediate_size": 11008,"max_position_embeddings": 4096,"num_attention_heads": 32,"num_hidden_layers": 32,"pad_token_id": 0,"vocab_size": 64000,"torch_dtype": "float32","rms_norm_eps": 1e-06,"tie_word_embeddings": false,"dropout_rate": 0.1,"learning_rate": 0.001,"batch_size": 32,"regularization": 0.01,"sequence_length": 512ã€‚" 
    }
]